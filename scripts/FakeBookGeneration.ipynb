{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Books for Training\n",
    "\n",
    "It is difficult to infer anthology contents and other whole/part relationships at large scales. This notebook generates fake books to mimic those relationships.\n",
    "\n",
    "Specifically, it generates:\n",
    "\n",
    "- Input: Multi-volume works; Output: Fake single volume of work\n",
    "- Input: A long single volume work; Output: Multiple fake volumes for parts of work (not yet implemented)\n",
    "- Input: Multiple works by the same author; Output: A 'works'-style anthology\n",
    "\n",
    "#### Workflow information\n",
    "As of Apr 2020, the process for running this is \n",
    "- Generate the files and ground truth by running this notebook\n",
    "- Convert the `fake` files to Vector_files, using `vectorization.py`\n",
    "- Concatenate the Vector_files with the real data, using the concatenate script in compare_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/saddl/meta.db'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compare_tools.configuration import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from compare_tools import fakebook\n",
    "from compare_tools.hathimeta import clean_description, HathiMeta\n",
    "from htrc_features import Volume\n",
    "# Use testset parameters\n",
    "test = False\n",
    "config.update(config['test' if test else 'full'])\n",
    "fakebook_root = '/data/saddl/fakebooks_{}set/'.format('test' if test else 'full')\n",
    "os.makedirs(fakebook_root, exist_ok=True)\n",
    "config['metadb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10      44.0\n",
       "0.25     120.0\n",
       "0.40     214.0\n",
       "0.50     278.0\n",
       "0.75     486.0\n",
       "0.90     746.0\n",
       "0.95     934.0\n",
       "0.99    1368.0\n",
       "Name: page_count, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hathimeta = HathiMeta(config['metadb_path'])\n",
    "meta = hathimeta.get_fields(fields=['htid', 'author', 'title', 'description', 'page_count'])\n",
    "meta.page_count = pd.to_numeric(meta.page_count)\n",
    "ground_truth = []\n",
    "meta.page_count.quantile([.1,.25,.4,.5,.75, .9, .95, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Books\n",
    "\n",
    "Antology Criteria - multiple works by an author that are different and relatively short.\n",
    "\n",
    "Combined Volumes criteria - multiple works that look to be parts of a sequential set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthologies\n",
    "\n",
    "Choose one book and patch it with other books where the title seems notably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial pool size is 757862 with n unique authors: 106986\n",
      "Final pool size is 143018 with n unique authors: 20000\n"
     ]
    }
   ],
   "source": [
    "# Create a pool of shorter books by authors with >2 books\n",
    "pool = meta[meta.page_count < meta.page_count.quantile(.4)]\n",
    "pool = pool.drop_duplicates(['author', 'title'])\n",
    "acounts = pool.groupby('author').title.count()\n",
    "pool = pool[pool.author.isin(acounts[acounts > 2].index)]\n",
    "nauthors = pool.author.unique().shape[0]\n",
    "print(\"Initial pool size is\", pool.shape[0], \"with n unique authors:\", pool.author.unique().shape[0])\n",
    "# trim pool when input data is extremely large.\n",
    "authors = pool.author.unique()\n",
    "np.random.shuffle(authors)\n",
    "pool = pool[pool.author.isin(authors[:20000])]\n",
    "print(\"Final pool size is\", pool.shape[0], \"with n unique authors:\", pool.author.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 1 author and return all their books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Abraham, Ralph.\n",
      "200 Adams, Paul W.\n",
      "300 Agg, John, 1783-1855.\n",
      "400 Aït-Sahalia, Yacine\n",
      "500 Alderman, Geoffrey.\n",
      "600 Allen, Daniel B.\n",
      "700 Alvarez Bravo, Manuel, 1902-2002.\n",
      "800 American College of Preventive Medicine.\n",
      "900 American Meat Institute Foundation.\n",
      "1000 Amherst college. Class of 1883.\n",
      "1100 Anderson, Michael, 1954-\n",
      "1200 Anquandah, James.\n",
      "1300 Ardell, Donald B.\n",
      "1400 Arnold, J. E. M.\n",
      "1500 Asia Society.\n",
      "1600 Atkinson, J. C. 1814-1900.\n",
      "1700 Australia\n",
      "1800 Babcock, Harold Lester, 1886-1953.\n",
      "1900 Bailey, H. W. 1899-1996.\n",
      "2000 Baker, Will, 1935-2005.\n",
      "2100 Baltimore (Md.)\n",
      "2200 Barker, Ernest, Sir, 1874-1960.\n",
      "2300 Barrass, Edward, 1821-1898.\n",
      "2400 Barton, Brigid S.\n",
      "2500 Baudelaire, Charles, 1821-1867.\n",
      "2600 Beard, James, 1903-1985.\n",
      "2700 Beesly, Edward Spencer, 1831-1915.\n",
      "2800 Ben-Yami, M.\n",
      "2900 Bennis, Phyllis, 1951-\n",
      "3000 Berke, Harry L.\n",
      "3100 Bethke, Eunice.\n",
      "3200 Bienvenu, Millard J.\n",
      "3300 Birney, Earle, 1904-1995.\n",
      "3400 Blaine, Robert Gordon.\n",
      "3500 Blest, A. D.\n",
      "3600 Bochner, Mel, 1940-\n",
      "3700 Bond, Victor R., 1934-\n",
      "3800 Bosman, J. J\n",
      "3900 Bover, Olympia.\n",
      "4000 Bošković, Aleksandar.\n",
      "4100 Branley, Franklyn M. 1915-2002.\n",
      "4200 Breukelman, John, 1901-\n",
      "4300 British Coal Utilisation Research Association.\n",
      "4400 Bronkhorst, Johannes, 1946-\n",
      "4500 Brown, Byron W., 1939-\n",
      "4600 Brown, Ronald D.\n",
      "4700 Brune, Lester H.\n",
      "4800 Buckminster, J. S. 1784-1812.\n",
      "4900 Burch, J. B. 1929-\n",
      "5000 Burness, Donald.\n",
      "5100 Bush, George Gary, 1843-1898.\n",
      "5200 Byerly, William Elwood, 1849-\n",
      "5300 Cairns, Stephen D. 1949-\n",
      "5400 California. Department of Transportation.\n",
      "5500 California. Legislature. Senate. Committee on Health and Welfare.\n",
      "5600 Calomiris, Charles W\n",
      "5700 Campbell-Tipton, Louis, 1877-1921.\n",
      "5800 Canaday, John, 1907-1985.\n",
      "5900 Caravaggio, Michelangelo Merisi da, 1573-1610.\n",
      "6000 Carpenter, Lant, 1780-1840.\n",
      "6100 Carter, Robert Goldthwaite, 1845-1936.\n",
      "6200 Cattell, Raymond B. 1905-1998.\n",
      "6300 Cesa, Edward T.\n",
      "6400 Chanda, S. N., 1931-\n",
      "6500 Charney, Melvin.\n",
      "6600 Cheshire, Frank Richard, 1833?-1894.\n",
      "6700 Chipasula, Frank Mkalawile.\n",
      "6800 Church of England. Diocese of Nova Scotia. Bishop (1787-1816 : Inglis)\n",
      "6900 Clark, Barrett H. 1890-1953.\n",
      "7000 Clarke, Frank Wigglesworth, 1847-1931.\n",
      "7100 Cliff, Tony.\n",
      "7200 Coffey, Wayne R.\n",
      "7300 Coleman, Charles H. 1900-\n",
      "7400 Collins, W. Lucas 1817-1887\n",
      "7500 Comer, David J.\n",
      "7600 Conference of Ministers Responsible for the Application of Science and Technology to Development in Latin America and the Caribbean 1985 : Brasilia, Brazil)\n",
      "7700 Conway, Richard Walter, 1931-\n",
      "7800 Coote, Anna.\n",
      "7900 Cornell University. Center for Environmental Quality Management.\n",
      "8000 Council on Library and Information Resources.\n",
      "8100 Craig, John A. 1864?-1910.\n",
      "8200 Crickmay, C. H. 1899-\n",
      "8300 Crump, Barry.\n",
      "8400 Curry, B. E.\n",
      "8500 Daim bin Zainuddin.\n",
      "8600 Daniel, James, 1967-\n",
      "8700 Daumal, René, 1908-1944.\n",
      "8800 Davis, Francis.\n",
      "8900 Day Lewis, C. 1904-1972.\n",
      "9000 DeFleur, Melvin L. 1923-\n",
      "9100 Dehn, Paul.\n",
      "9200 Dennys, John, fl. 1609.\n",
      "9300 Dexter, Franklin Bowditch, 1842-1920.\n",
      "9400 Dimbleby, G. W.\n",
      "9500 Dodge, Raymond, 1871-1942.\n",
      "9600 Dougherty, Lawrence A.\n",
      "9700 Drawbridge, Cyprian Leycester, 1868-\n",
      "9800 Duffield, John T. 1823-1901.\n",
      "9900 Dupre, Huntley, b. 1892.\n",
      "10000 Ealy, Robert P. 1914-\n",
      "10100 Economist Intelligence Unit (Great Britain)\n",
      "10200 Egan, Gerard.\n",
      "10300 Elder, William, 1806-1885.\n",
      "10400 Elphinston, James, 1721-1809.\n",
      "10500 Eno Foundation for Highway Traffic Control.\n",
      "10600 Estanislao, Jesus P.\n",
      "10700 Evans, Robert Jones, 1863-\n",
      "10800 Fairbairn, William, Sir, 1789-1874.\n",
      "10900 Farr, E. H.\n",
      "11000 Fehl, Noah Edward, 1917-\n",
      "11100 Fernando, Mervyn.\n",
      "11200 Finan, William F\n",
      "11300 Fischer, Stanley.\n",
      "11400 Fitzgerald, Frank M.\n",
      "11500 Foged, Niels\n",
      "11600 Ford, William F. 1934-\n",
      "11700 Fournier, Paul G.\n",
      "11800 Frandsen, William H.\n",
      "11900 Freeman, Stan.\n",
      "12000 Frisch, D. H.\n",
      "12100 Fuller, J. F. C. 1878-1966.\n",
      "12200 Gallagher, J. Roswell 1903-\n",
      "12300 Garman, Philip, 1891-\n",
      "12400 Gayle, John B.\n",
      "12500 Geological Survey of California.\n",
      "12600 Ghent, William J. 1866-1942.\n",
      "12700 Gilbert, Francis C.\n",
      "12800 Gilpatric, Carolyn Draper.\n",
      "12900 Glubok, Shirley.\n",
      "13000 Goldman, Judith.\n",
      "13100 Goodman, Nelson.\n",
      "13200 Gorham, George C. 1832-1909.\n",
      "13300 Graham, Benjamin, 1894-1976.\n",
      "13400 Gray, Elaine.\n",
      "13500 Great Britain. Ministry of Reconstruction.\n",
      "13600 Greene, Henry Copley, 1871-\n",
      "13700 Grenfell, Bernard P. 1869-1926.\n",
      "13800 Grindle, Merilee Serrill.\n",
      "13900 Guardini, Romano, 1885-1968.\n",
      "14000 Guru Dutt, K.\n",
      "14100 Hadwen, Walter R. 1854-1932.\n",
      "14200 Hall, Archibald, 1813-1868.\n",
      "14300 Hamilton, Charles V.\n",
      "14400 Hanks, Lucien M. 1910-\n",
      "14500 Hardy, Charles O. 1884-1948.\n",
      "14600 Harris, Edward M.\n",
      "14700 Hartford (Conn.). Court of Common Council.\n",
      "14800 Hassan, Ahmad A.\n",
      "14900 Hawley, Charles, 1819-1885.\n",
      "15000 Heap, Christine.\n",
      "15100 Hellman, Hal, 1927-\n",
      "15200 Hentoff, Nat.\n",
      "15300 Hest, Amy.\n",
      "15400 Hildreth, Gertrude Howell, 1898-1984.\n",
      "15500 Hines, F. Dee.\n",
      "15600 Hobbs, Robert Carleton, 1946-\n",
      "15700 Hogarth, George, 1783-1870.\n",
      "15800 Holmer, Nils Magnus, 1904-1994\n",
      "15900 Hoos, Sidney Samuel, 1911-,\n",
      "16000 Horton, Jerome S. 1910-\n",
      "16100 Howard, Michael, 1922-\n",
      "16200 Hubbell, George Allen, 1862-\n",
      "16300 Humbach, Helmut, 1921-\n",
      "16400 Hurlbert, J. Beaufort\n",
      "16500 Hyman, Eric.\n",
      "16600 Illinois State Normal University.\n",
      "16700 India.\n",
      "16800 Indiana. Dept. of public instruction. [from old catalog]\n",
      "16900 Institute of Museum and Library Services (U.S.)\n",
      "17000 International Foundation for Research in the Field of Advertising.\n",
      "17100 Interstate Conference of Employment Security Agencies. Benefit Financing Committee.\n",
      "17200 Irwin, Richard.\n",
      "17300 Jackson, Howard, 1945-\n",
      "17400 Jalava, Väinö.\n",
      "17500 Jasmin, Claude, 1930-\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for groupname, subset in pool.groupby('author'):\n",
    "    to_combine1 = fakebook.anthology_sample(subset)\n",
    "    to_combine2 = fakebook.anthology_sample(subset)\n",
    "    \n",
    "    new_ids = []\n",
    "    # Check if there's an OVERLAP relationship\n",
    "    overlap = set(to_combine1).intersection(to_combine2)\n",
    "    if (len(overlap) < 1) or (len(overlap) > 2):\n",
    "        # Don't bother making the second fake doc\n",
    "        to_combine2 = []\n",
    "    \n",
    "    for to_combine in [to_combine1, to_combine2]:\n",
    "        if len(to_combine) > 1:\n",
    "            try:\n",
    "                volmeta, tl = fakebook.combine_books(to_combine)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            new_ids.append(volmeta['id'])\n",
    "            fakebook.save_fake_vol(volmeta, tl, fakebook_root)\n",
    "            for source_htid in volmeta['source_htids']:\n",
    "                ground_truth.append(dict(left=volmeta['id'], right=source_htid, judgment='CONTAINS', notes='fake anthology'))\n",
    "                ground_truth.append(dict(left=source_htid, right=volmeta['id'], judgment='PARTOF', notes='fake anthology'))\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(i, groupname)\n",
    "            \n",
    "    if len(new_ids) == 2:\n",
    "        ground_truth.append(dict(left=new_ids[1], right=new_ids[0], judgment='OVERLAPS', notes='fake overlap'))\n",
    "        ground_truth.append(dict(left=new_ids[0], right=new_ids[1], judgment='OVERLAPS', notes='fake overlap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-volume sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113382, 6)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = meta[meta.page_count < meta.page_count.quantile(.6)]\n",
    "pool = pool[~pool.description.isna()]\n",
    "pool = pool[clean_description(pool.description).str.contains('^v\\.\\d\\d?$')]\n",
    "# Filter to author/title pairs that have more than one volume\n",
    "pool = pool.groupby(['author', 'title']).filter(lambda x: x.description.unique().shape[0] > 1)\n",
    "pool = pool.copy()\n",
    "pool['descint'] = clean_description(pool.description).str.replace('v.', '').astype(int)\n",
    "# Filter further, to author/title pairs that have consecutively numbered volumes\n",
    "def has_consecutive_v(x):\n",
    "    sorted_v_ints = x.descint.sort_values()\n",
    "    cumulative_run_length = ((sorted_v_ints - 1) == sorted_v_ints.shift(1))\n",
    "    return cumulative_run_length.any()\n",
    "pool = pool.groupby(['author', 'title']).filter(has_consecutive_v)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ('Adams, Ansel, 1902-1984', 'Basic photo.')\n",
      "200 ('Aikin, Lucy, 1781-1864.', 'The life of Joseph Addison. By Lucy Aikin ...')\n",
      "300 ('Aldrich, Thomas Bailey, 1836-1907.', 'The poems of Thomas Bailey Aldrich.')\n",
      "400 ('Allen, William, 1532-1594.', 'A true, sincere and modest defence of English Catholics that suffer for their faith both at home and abroad, against a false, seditious and slanderous libel, entitled: \"The execution of justice in England\" / by William Allen ; with a preface by his eminence the Cardinal Archbishop of Westminister.')\n",
      "500 ('American Institute of Chemical Engineers.', 'Nuclear engineering.')\n",
      "600 ('American Sociological Association', 'Publication of the American Sociological Society')\n",
      "700 ('Anjou, Gustave, 1863-1942', \"Ulster County, N.Y. probate records in the office of the surrogate, and in the county clerk's office at Kingston, N.Y. A careful abstract and    translation of the Dutch and English wills, letters of administration after      intestates, and inventories from l665, with genealogical and historical notes,  and list of Dutch and Frisian baptismal names with their English equivalents / by Gustave Anjou. With introduction by Judge A. T. Clearwater.\")\n",
      "800 ('Arlinghaus, Sandra L.', 'Essays on mathematical geography / by Sandra L. Arlinghaus.')\n",
      "900 ('Ashbee, Janet E.', 'The Essex House song book : being the collection of songs formed for the singers of the Guild of Handicraft / by C.R. and Janet E. Ashbee, and edited by her.')\n",
      "1000 ('Auerbach, Berthold, 1812-1882.', 'On the heights / by Berthold Auerbach ; translated by F. E. Bunnett. v.1')\n",
      "1100 ('Ayrshire and Galloway Archaeological Association.', 'Archæological and historical collections relating to Ayrshire & Galloway.')\n",
      "1200 ('Baird, S. W.', 'Graded Work in Arithmetic. Years. I-IV.')\n",
      "1300 ('Bancroft, Edward, 1744-1821.', 'The history of Charles Wentworth, Esq. / Edward Bancroft.')\n",
      "1400 ('Barker, Bernard.', 'Eliot the younger; a fiction in freehand.')\n",
      "1500 ('Baruch, Simon, 1840-1921.', 'The uses of water in modern medicine. By Simon Baruch ...')\n",
      "1600 ('Beattie, James, 1735-1803.', 'Dissertations moral and critical.')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "early_stop = 20000\n",
    "for groupname, subset in pool.groupby(['author', 'title']):\n",
    "    smaller_subset = subset.copy().groupby('descint').apply(lambda x: x.sample(1))\n",
    "    for to_combine in fakebook.consecutive_vol_samples(smaller_subset):\n",
    "        if len(to_combine) > 1:\n",
    "            try:\n",
    "                volmeta, tl = fakebook.combine_books(to_combine, style='multivol')\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                continue\n",
    "            fakebook.save_fake_vol(volmeta, tl, fakebook_root)\n",
    "            for source_htid in volmeta['source_htids']:\n",
    "                ground_truth.append(dict(left=volmeta['id'], right=source_htid, judgment='CONTAINS', notes='fake multivol'))\n",
    "                ground_truth.append(dict(left=source_htid, right=volmeta['id'], judgment='PARTOF', notes='fake multivol'))\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, groupname)\n",
    "    if i % early_stop == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ground_truth)\n",
    "df.to_parquet(fakebook_root + 'fakebook_gt.parquet')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = df.left[df.left.str.startswith('fake')].drop_duplicates()\n",
    "to_save.name = 'htid'\n",
    "to_save.to_csv(fakebook_root+'fake-htids.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stat crunching list\n",
    "import json\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(fakebook_root + 'fakebook_gt.parquet')\n",
    "with open(fakebook_root + 'stat_input.json', mode='w') as f:\n",
    "    for record in df.sort_values('left').to_dict(orient='records'):\n",
    "        json.dump(record, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up\n",
    "\n",
    "Two things remain to be done: vector_file versions of the fake books need to be created (the list of ids is in `fake-htids.csv`), and the classifier input files need to be crunched. Code for these actions is in `../workflow.md`. Remember that vectorization setting should be consistent with the 'real' files - currently I'm using 300d gigaword GloVe with 5k chunks.\n",
    "\n",
    "Parallelized:\n",
    "```\n",
    "mkdir -p /tmp/fake\n",
    "seq 1 20 | parallel --eta -n1 -j20 python vectorization.py --outdir /tmp/fake --no-srp -g 50 --chunksize 5000 {} 20 /data/saddl/fakebooks_testset/fake-htids.csv\n",
    "python concatenate-vector_files.py --build-cache --mode w /data/vectorfiles/fake_testset.bin /tmp/fake/*bin\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
