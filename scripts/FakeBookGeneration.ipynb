{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Books for Training\n",
    "\n",
    "It is difficult to infer anthology contents and other whole/part relationships at large scales. This notebook generates fake books to mimic those relationships.\n",
    "\n",
    "Specifically, it generates:\n",
    "\n",
    "- Input: Multi-volume works; Output: Fake single volume of work\n",
    "- Input: A long single volume work; Output: Multiple fake volumes for parts of work (not yet implemented)\n",
    "- Input: Multiple works by the same author; Output: An 'works'-style anthology\n",
    "\n",
    "#### Workflow information\n",
    "As of Apr 2020, the process for running this is \n",
    "- Generate the files and ground truth by running this notebook\n",
    "- Convert the `fake` files to Vector_files, using `vectorization.py`\n",
    "- Concatenate the Vector_files with the real data, using the concatenate script in compare_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10     56.0\n",
       "0.20    124.0\n",
       "0.30    192.0\n",
       "0.40    248.0\n",
       "0.50    302.0\n",
       "0.60    352.0\n",
       "0.70    410.0\n",
       "0.75    444.0\n",
       "0.80    488.0\n",
       "0.85    538.0\n",
       "0.90    612.0\n",
       "0.95    742.0\n",
       "Name: page_count, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from compare_tools import fakebook\n",
    "from compare_tools.hathimeta import clean_description\n",
    "from htrc_features import Volume\n",
    "meta = pd.read_csv('../../sampling/test_dataset.csv.gz', low_memory=False)\n",
    "ground_truth = []\n",
    "meta.page_count.quantile([.1,.2,.3,.4,.5,.6,.7,.75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Books\n",
    "\n",
    "Antology Criteria - multiple works by an author that are different and relatively short.\n",
    "\n",
    "Combined Volumes criteria - multiple works that look to be parts of a sequential set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthologies\n",
    "\n",
    "Choose one book and patch it with other books where the title seems notably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39124, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = meta[meta.page_count < meta.page_count.quantile(.4)]\n",
    "pool = pool.drop_duplicates(['author', 'title'])\n",
    "acounts = pool.groupby('author').title.count()\n",
    "pool = pool[pool.author.isin(acounts[acounts > 2].index)]\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 1 author and return all their books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 American Society for Testing and Materials.\n",
      "200 Balfour, Clara Lucas, 1808-1878.\n",
      "300 Bennett, Edward H. 1874-1954.\n",
      "400 Bord, Janet, 1945-\n",
      "500 Brooks, William Keith, 1848-1908.\n",
      "600 Calhoun, John C. 1782-1850.\n",
      "700 Cherrington, Ernest Hurst, 1877-1950.\n",
      "800 Conference of Local and Regional Authorities of Europe.\n",
      "900 Currimbhoy, Asif, 1928-\n",
      "1000 Dixwell, George Basil.\n",
      "1100 Eells, Myron, 1843-1907\n",
      "1200 Field, Frank, 1942-\n",
      "1300 Fullerton, Georgiana, Lady, 1812-1885.\n",
      "1400 Gonzalez-Mena, Janet.\n",
      "1500 Gunning, Mrs. 1740?-1800.\n",
      "1600 Haviland, Virginia, 1911-\n",
      "1700 Holmes, Edmond Gore Alexander, 1850-1936.\n",
      "1800 India. Committee on Plan Projects. Minor Irrigation Team.\n",
      "1900 Johnson, Lionel, 1867-1902.\n",
      "2000 Ker, W. P. 1855-1923.\n",
      "2100 Lang, Jeanie.\n",
      "2200 London, Barbara, 1936-\n",
      "2300 Mao, Zedong, 1893-1976.\n",
      "2400 McKinley, Albert E. 1870-1936.\n",
      "2500 Montale, Eugenio, 1896-1981.\n",
      "2600 National Research Council (U.S.). Committee on Oceanography.\n",
      "2700 Nystrom, John W. 1824-1885.\n",
      "2800 Paton, Lewis Bayles, 1864-1932.\n",
      "2900 Pollard, James Edward, 1894-\n",
      "3000 Rathbone, Josephine Langworthy, 1899-\n",
      "3100 Rostow, W. W. 1916-2003.\n",
      "3200 Schmitz, Robert Morell, 1900-\n",
      "3300 Shull, A. Franklin b. 1881.\n",
      "3400 Southern Railway (U.S.)\n",
      "3500 Sturtevant, A. H. 1891-1970.\n",
      "3600 Thorner, Daniel.\n",
      "3700 United Nations. Security Council.\n",
      "3800 Vignola, 1507-1573.\n",
      "3900 Wendleton, Kate.\n",
      "4000 Wise, John, 1652-1725.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for groupname, subset in pool.groupby('author'):\n",
    "    to_combine = fakebook.anthology_sample(subset)\n",
    "    if len(to_combine) > 1:\n",
    "        try:\n",
    "            volmeta, tl = fakebook.combine_books(to_combine)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            continue\n",
    "        fakebook.save_fake_vol(volmeta, tl, '/data/saddl/fakebooks/')\n",
    "\n",
    "        for source_htid in volmeta['source_htids']:\n",
    "            ground_truth.append(dict(left=volmeta['id'], right=source_htid, judgment='CONTAINS', notes='fake anthology'))\n",
    "            ground_truth.append(dict(left=source_htid, right=volmeta['id'], judgment='PARTOF', notes='fake anthology'))\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, groupname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-volume sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5051, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = meta[meta.page_count < meta.page_count.quantile(.6)]\n",
    "pool = pool[~pool.description.isna()]\n",
    "pool = pool[clean_description(pool.description).str.contains('^v\\.\\d\\d?$')]\n",
    "# Filter to author/title pairs that have more than one volume\n",
    "pool = pool.groupby(['author', 'title']).filter(lambda x: x.description.unique().shape[0] > 1)\n",
    "pool = pool.copy()\n",
    "pool['descint'] = clean_description(pool.description).str.replace('v.', '').astype(int)\n",
    "# Filter further, to author/title pairs that have consecutively numbered volumes\n",
    "def has_consecutive_v(x):\n",
    "    sorted_v_ints = x.descint.sort_values()\n",
    "    cumulative_run_length = ((sorted_v_ints - 1) == sorted_v_ints.shift(1))\n",
    "    return cumulative_run_length.any()\n",
    "pool = pool.groupby(['author', 'title']).filter(has_consecutive_v)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ('Bond, Francis, d. 1918.', 'Wood carvings in English churches, by Francis Bond.')\n",
      "200 ('Casanova, Giacomo, 1725-1798.', 'The memoirs of Jacques Casanova de Seingalt. / Complete in twelve volumes as translated into English by Arthur Machen with an introd. by Arthur Symons, a new pref. by the translator and twelve drawings by Rockwell Kent.')\n",
      "300 ('Edwards, Amelia Ann Blanford, 1831-1892.', \"Barbara's history / Amelia B. Edwards ...\")\n",
      "400 ('Government Oriental Manuscripts Library (Tamil Nadu, India)', 'A descriptive catalogue of the Islamic manuscripts in the Government Oriental manuscripts library, Madras, by Vidyasagara Vidyavacaspati P. P. Subrahmanya Sastri. (Prepared under the orders of the government of Madras)')\n",
      "500 ('Hyams, Edward, 1910-1975.', 'Ornamental shrubs for temperate zone gardens.')\n",
      "600 ('Knox, Vicesimus, 1752-1821.', 'Essays, moral and literary / Vicesimus Knox.')\n",
      "700 ('Matematicheskiĭ institut im. V.A. Steklova.', 'Mathematics, its content, methods and meaning. Edited by A. D. Aleksandrov, A. N. Kolmogorov [and] M. A. Lavrentʾev.')\n",
      "800 (\"Orléans, Charlotte-Elisabeth, duchesse d', 1652-1722.\", 'The letters of Madame : the correspondence of Elizabeth-Charlotte of Bavaria, princess palatine, duchess of Orleans, called \"Madame\" at the court of King Louis XIV / translated and edited by Gertrude Scott Stevenson, M. A.')\n",
      "900 ('Robinson, C. W. 1836-1924.', \"Wellington's campaigns, Peninsula-Waterloo, 1808-15; also Moore's campaign of Corunna (for military students)\")\n",
      "1000 ('Sullivan, Arthur, 1842-1900.', 'The Savoy operas : being the complete text of the Gilbert and Sullivan operas as originally produced in the years 1875-1896 / by Sir W. S. Gilbert.')\n",
      "1100 ('Wilde, Oscar, 1854-1900.', 'The writings of Oscar Wilde ...')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for groupname, subset in pool.groupby(['author', 'title']):\n",
    "    smaller_subset = subset.copy().groupby('descint').apply(lambda x: x.sample(1))\n",
    "    for to_combine in fakebook.consecutive_vol_samples(smaller_subset):\n",
    "        if len(to_combine) > 1:\n",
    "            try:\n",
    "                volmeta, tl = fakebook.combine_books(to_combine, style='multivol')\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                continue\n",
    "            fakebook.save_fake_vol(volmeta, tl, '/data/saddl/fakebooks/')\n",
    "            for source_htid in volmeta['source_htids']:\n",
    "                ground_truth.append(dict(left=volmeta['id'], right=source_htid, judgment='CONTAINS', notes='fake multivol'))\n",
    "                ground_truth.append(dict(left=source_htid, right=volmeta['id'], judgment='PARTOF', notes='fake multivol'))\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, groupname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>judgment</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>fake.bf8854</td>\n",
       "      <td>mdp.39015003501320</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31555</th>\n",
       "      <td>mdp.39015081108246</td>\n",
       "      <td>fake.548187</td>\n",
       "      <td>PARTOF</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10058</th>\n",
       "      <td>fake.ebb499</td>\n",
       "      <td>coo.31924000520464</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15882</th>\n",
       "      <td>fake.7484fa</td>\n",
       "      <td>loc.ark:/13960/t8jd5bz53</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29820</th>\n",
       "      <td>fake.5255cc</td>\n",
       "      <td>uc1.c055928877</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>fake.cbf30a</td>\n",
       "      <td>mdp.39015033362438</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26526</th>\n",
       "      <td>fake.68107d</td>\n",
       "      <td>umn.319510007805824</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32277</th>\n",
       "      <td>umn.31951002005585i</td>\n",
       "      <td>fake.e2f490</td>\n",
       "      <td>PARTOF</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30788</th>\n",
       "      <td>fake.994d91</td>\n",
       "      <td>loc.ark:/13960/t9086xs0s</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake anthology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38354</th>\n",
       "      <td>fake.3a22be</td>\n",
       "      <td>umn.31951d00699948u</td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>fake multivol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      left                     right  judgment           notes\n",
       "8308           fake.bf8854        mdp.39015003501320  CONTAINS  fake anthology\n",
       "31555   mdp.39015081108246               fake.548187    PARTOF  fake anthology\n",
       "10058          fake.ebb499        coo.31924000520464  CONTAINS  fake anthology\n",
       "15882          fake.7484fa  loc.ark:/13960/t8jd5bz53  CONTAINS  fake anthology\n",
       "29820          fake.5255cc            uc1.c055928877  CONTAINS  fake anthology\n",
       "27970          fake.cbf30a        mdp.39015033362438  CONTAINS  fake anthology\n",
       "26526          fake.68107d       umn.319510007805824  CONTAINS  fake anthology\n",
       "32277  umn.31951002005585i               fake.e2f490    PARTOF  fake anthology\n",
       "30788          fake.994d91  loc.ark:/13960/t9086xs0s  CONTAINS  fake anthology\n",
       "38354          fake.3a22be       umn.31951d00699948u  CONTAINS   fake multivol"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ground_truth)\n",
    "df.to_parquet('/data/saddl/fakebooks/fakebook_gt.parquet')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left[df.left.str.startswith('fake')].drop_duplicates().to_csv('/data/saddl/fakebooks/fake-htids.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
