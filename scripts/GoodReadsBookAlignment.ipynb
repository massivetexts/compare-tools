{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/massivetexts/compare-tools/blob/master/scripts/GoodReadsBookAlignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLDXH9sOu1NL"
   },
   "source": [
    "This notebook cross-references HathiTrust and GoodReads (via the USCD dataset), to find 'similar to' relationships. This can be used for training a different type of contextual relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFXF7-yhezzI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "Oz2ISjM8gJhg",
    "outputId": "3bf2b077-8f20-4024-e412-11dac42d06c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/7a/5a892d25b0a105b9d825cdd0c69d2742b83e00f247053f50a7e0a5405439/gdown-3.11.1.tar.gz\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/ec/6ee2168387ce0154632f856d5cc5592328e9cf93127c5c9aeca92c8c16cb/filelock-3.0.12.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests[socks] in /usr/lib/python2.7/site-packages (from gdown)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages (from gdown)\n",
      "Collecting tqdm (from gdown)\n",
      "  Using cached https://files.pythonhosted.org/packages/af/88/7b0ea5fa8192d1733dea459a9e3059afc87819cb4072c43263f2ec7ab768/tqdm-4.48.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied (use --upgrade to upgrade): certifi>=2017.4.17 in /usr/lib/python2.7/site-packages (from requests[socks]->gdown)\n",
      "Requirement already satisfied (use --upgrade to upgrade): chardet<3.1.0,>=3.0.2 in /usr/lib/python2.7/site-packages (from requests[socks]->gdown)\n",
      "Requirement already satisfied (use --upgrade to upgrade): idna<2.9,>=2.5 in /usr/lib/python2.7/site-packages (from requests[socks]->gdown)\n",
      "Requirement already satisfied (use --upgrade to upgrade): urllib3<1.25,>=1.21.1 in /usr/lib/python2.7/site-packages (from requests[socks]->gdown)\n",
      "Requirement already satisfied (use --upgrade to upgrade): PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/lib/python2.7/site-packages (from requests[socks]->gdown)\n",
      "Installing collected packages: filelock, tqdm, gdown\n",
      "  Running setup.py install for filelock ... \u001b[?25lerror\n",
      "    Complete output from command /usr/bin/python2 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Hukvz6/filelock/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-lKn6HX-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib\n",
      "    copying filelock.py -> build/lib\n",
      "    running install_lib\n",
      "    copying build/lib/filelock.py -> /usr/lib/python2.7/site-packages\n",
      "    error: /usr/lib/python2.7/site-packages/filelock.py: Permission denied\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/usr/bin/python2 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Hukvz6/filelock/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-lKn6HX-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-Hukvz6/filelock/\u001b[0m\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h/bin/sh: gdown: command not found\n",
      "/bin/sh: gdown: command not found\n"
     ]
    }
   ],
   "source": [
    "#@title Download Dataset Files - HathiFiles and USCD Book Data\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1LXpK1UfqtP89H1tYy0pBGHjYk8IhigUK\n",
    "!gdown https://drive.google.com/uc?id=19cdwyXwfXx_HDIgxXaHzH0mrx8nMyLvC\n",
    "!wget -O hathifiles.tsv.gz https://www.hathitrust.org/filebrowser/download/291721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "ZhnQkqUNmWCZ",
    "outputId": "4cf08f7a-11fe-47b4-f1b7-c892f62d9f84"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive, for saving derived data.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    GRANT_FOLDER = '/content/drive/My Drive/Grants/IMLS Grant/Data/' #@param {type:'string'}\n",
    "except:\n",
    "    GRANT_FOLDER = '/data/saddl/stats/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z__kl7OLVwn7"
   },
   "source": [
    "## Read HathiTrust Metadata\n",
    "\n",
    "Due to the size of the Hathifiles, create a chunk iterator, that only reads a part of the full dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "4dExlX_ofSxQ",
    "outputId": "487ed4f4-6477-4b76-c2b2-c4129cabad75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>htid</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>rights_date_used</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45417</th>\n",
       "      <td>mdp.39015016041785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spectroscopy in heterogeneous catalysis / by W...</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>mdp.39015019059933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The teaching of contempt; Christian roots of a...</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>Isaac, Jules, 1877-1963.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76462</th>\n",
       "      <td>iau.31858029317207</td>\n",
       "      <td>ser.3 v.89 Jan.19-Feb.15 1847</td>\n",
       "      <td>Hansard's parliamentary debates.</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>Great Britain. Parliament.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82790</th>\n",
       "      <td>mdp.39015080115507</td>\n",
       "      <td>no.17</td>\n",
       "      <td>Proces-verbaal van de ... algemeene vergaderin...</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>Koninklijk Instituut van Wetenschappen, Letter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     htid                    description  \\\n",
       "45417  mdp.39015016041785                            NaN   \n",
       "8509   mdp.39015019059933                            NaN   \n",
       "76462  iau.31858029317207  ser.3 v.89 Jan.19-Feb.15 1847   \n",
       "82790  mdp.39015080115507                          no.17   \n",
       "\n",
       "                                                   title  rights_date_used  \\\n",
       "45417  Spectroscopy in heterogeneous catalysis / by W...            1979.0   \n",
       "8509   The teaching of contempt; Christian roots of a...            1964.0   \n",
       "76462                   Hansard's parliamentary debates.            1847.0   \n",
       "82790  Proces-verbaal van de ... algemeene vergaderin...            1846.0   \n",
       "\n",
       "                                                  author  \n",
       "45417                                                NaN  \n",
       "8509                            Isaac, Jules, 1877-1963.  \n",
       "76462                         Great Britain. Parliament.  \n",
       "82790  Koninklijk Instituut van Wetenschappen, Letter...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings = ['htid', 'access', 'rights', 'ht_bib_key', 'description', 'source', \n",
    "            'source_bib_num', 'oclc_num', 'isbn', 'issn', 'lccn', 'title', \n",
    "            'imprint', 'rights_reason_code', 'rights_timestamp', 'us_gov_doc_flag', \n",
    "            'rights_date_used', 'pub_place', 'lang', 'bib_fmt', 'collection_code', \n",
    "            'content_provider_code', 'responsible_entity_code', \n",
    "            'digitization_agent_code', 'access_profile_code', 'author']\n",
    "# Dataset is large, so use an iterator\n",
    "htreader = pd.read_csv('hathifiles.tsv.gz', sep='\\t', compression='gzip',\n",
    "                     chunksize=100000, names=headings,\n",
    "                     usecols=['htid', 'author', 'title', 'rights_date_used',\n",
    "                               'description'])\n",
    "for htmeta in htreader:\n",
    "    example = htmeta.sample(4)\n",
    "    break\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pc29LaaKixbD"
   },
   "source": [
    "## Loading USCD data\n",
    "- read dataset file content (needs decompression)\n",
    "- parse json from content\n",
    "- loop through the books, and for each book, see if there is a match in our HathiTrust metadata DataFrame\n",
    "\n",
    "### Load Author Data and Cross-reference with HT\n",
    "\n",
    "First, we want to find the authors that are possible in the HathiTrust. Those that are not, we can ignore.\n",
    "\n",
    "This is a tricky alignment, because the HathiTrust author data is somewhat messy in it's formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "-PM1kQcTHKHY",
    "outputId": "8edfdbe6-2ae6-4508-b7dc-df56008c3f63"
   },
   "outputs": [],
   "source": [
    "authors = pd.read_json('goodreads_book_authors.json.gz', compression='gzip', lines=True)\n",
    "authors.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "KwLXlTRoLRT7",
    "outputId": "3b83d956-4ff5-4fd7-e149-469b3b49a499"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>name</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>new_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>790587</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3438740</td>\n",
       "      <td>1</td>\n",
       "      <td>Hector Giacomelli</td>\n",
       "      <td>1</td>\n",
       "      <td>Giacomelli, Hector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65855</th>\n",
       "      <td>3.50</td>\n",
       "      <td>15581717</td>\n",
       "      <td>3</td>\n",
       "      <td>Gianluca Pirozzi</td>\n",
       "      <td>4</td>\n",
       "      <td>Pirozzi, Gianluca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343714</th>\n",
       "      <td>3.12</td>\n",
       "      <td>3078325</td>\n",
       "      <td>1</td>\n",
       "      <td>Victor Alvarez</td>\n",
       "      <td>8</td>\n",
       "      <td>Alvarez, Victor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        average_rating  author_id  ...  ratings_count            new_name\n",
       "790587            2.00    3438740  ...              1  Giacomelli, Hector\n",
       "65855             3.50   15581717  ...              4   Pirozzi, Gianluca\n",
       "343714            3.12    3078325  ...              8     Alvarez, Victor\n",
       "\n",
       "[3 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new lastname, firstname column in the USCD Authors Dataset\n",
    "authors['new_name'] = authors.name.str.replace('^(.*) ([A-Z].*)$', r'\\2, \\1')\n",
    "authors.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "UA0XUtaJMLYZ",
    "outputId": "9a8b25dd-46a8-48f2-a716-48497728d197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (16,25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124,125,126,127,128,129,130,131,132,133,134,135,136,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137,138,139,140,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3314666,), (8557627,))"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all the unique authors and unique book titles in the HathiTrust without\n",
    "# loading it all into memory\n",
    "\n",
    "import numpy as np\n",
    "all_ht_authors = []\n",
    "all_ht_titles = []\n",
    "for i, htmeta in enumerate(htreader):\n",
    "    print(i, end=',')\n",
    "    all_ht_authors.append(htmeta.author.unique())\n",
    "    all_ht_titles.append(htmeta.title.unique())\n",
    "print()\n",
    "all_ht_authors = pd.Series(np.concatenate(all_ht_authors)).drop_duplicates().fillna('')\n",
    "all_ht_titles = pd.Series(np.concatenate(all_ht_titles)).drop_duplicates().fillna('')\n",
    "all_ht_authors.shape, all_ht_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "X_GlzmI3MK0d",
    "outputId": "123e490c-f696-4b19-db80-af3784d74ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for Compare-Tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Reformat the author and book title information from the HathiTrust Metadata.\n",
    "# This won't be perfect, but we don't need completeness so the few places where \n",
    "# it messes up should be fine.\n",
    "!pip install -q git+git://github.com/massivetexts/compare-tools\n",
    "from compare_tools.hathimeta import clean_title\n",
    "\n",
    "def clean_author(x):\n",
    "    import re\n",
    "    x = re.sub('\\-?\\d\\d\\d\\d', '', x)\n",
    "    x = str(x).strip().strip('.')\n",
    "    x = x.split(',')[:2]\n",
    "    x = \",\".join(x)\n",
    "    return x\n",
    "\n",
    "def simple_title(x):\n",
    "    return (x.apply(clean_title)   # Run the compare_tools clean_title code\n",
    "             .str.lower()          # Lowercase\n",
    "             .str.split(r':|/|\\\\') # Split on :, /, \\\n",
    "             .apply(lambda x:x[0]) # Keep first string of split\n",
    "             .str.replace('\\W', '') # Only keep non-word chars\n",
    "             .apply(lambda x: x[:35]) # Truncate to first 35 chars\n",
    "    )\n",
    "all_ht_authors = all_ht_authors.apply(clean_author)\n",
    "clean_ht_titles = simple_title(all_ht_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "r5fqa3Q3X1bs",
    "outputId": "69613b8d-3bfe-4a6c-9ff6-9469bc26f336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174967    forandagainstthestate\n",
       "3647964              feudafrique\n",
       "2559463     stilidiemancipazione\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ht_titles.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tfvZ84YYqi9"
   },
   "source": [
    "Here, I truncate the `authors` data. I rewrite the original variables to save memory.\n",
    "no reason to hold the entire original dataset in memory any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "2Ytgc0C9MO2m",
    "outputId": "63bc17e4-1ee8-4b9a-dc34-049cb8cbafdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Author Intersection\n",
      "# of authors seen in both datasets: 120407\n",
      "Pre-size: 829529\n",
      "Post-size: 120927\n"
     ]
    }
   ],
   "source": [
    "# Cross reference the HT Meta and the Goodreads data to find authors in both\n",
    "print(\"Checking Author Intersection\")\n",
    "a = set(all_ht_authors)\n",
    "b = set(authors.new_name)\n",
    "overlap = b.intersection(a)\n",
    "print(\"# of authors seen in both datasets:\", len(overlap))\n",
    "\n",
    "# Truncate Authors table\n",
    "print(\"Pre-size:\", authors.shape[0])\n",
    "authors = authors[authors.new_name.isin(overlap)]\n",
    "print(\"Post-size:\", authors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tvpbxe3hZCjC"
   },
   "source": [
    "## Load Book information for authors that may be in the HathiTrust\n",
    "\n",
    "First, the book data needs to be joined with authors, which has already been truncated to authors that we can find in the HathiTrust.\n",
    "\n",
    "Then, we derive a cleaned title and only keep the rows where there it matches the unique cleaned titles in the HathiTrust. This is saved to an CSV File in Google Docs called `merge_books.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "ucHb2LcAHW-V",
    "outputId": "969b98a5-9c92-4e6b-dc76-5b6c228db817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cols_to_keep = ['isbn', 'popular_shelves','similar_books', 'average_rating', 'link',\n",
    "                'publication_year', 'book_id', 'title', 'title_without_series',\n",
    "                'work_id', 'author_id', 'author', 'author_formatted', 'simple_title',\n",
    "                'edition_information']\n",
    "\n",
    "bookreader = pd.read_json('goodreads_books.json.gz', compression='gzip',\n",
    "                          lines=True, chunksize=100000)\n",
    "\n",
    "outf = os.path.join(GRANT_FOLDER, 'merge_books.csv.gz')\n",
    "\n",
    "for i, books in enumerate(bookreader):\n",
    "    print(i, end=', ')\n",
    "    # Extract the first author\n",
    "    books['first_author_id'] = books.authors.apply(lambda x: x[0]['author_id'] if len(x) > 0 else -1).astype(int)\n",
    "\n",
    "    # Do an inner join with authors. This will shrink the dataset\n",
    "    merged = books.merge(authors[['name', 'new_name', 'author_id']], \n",
    "                        how='inner', left_on='first_author_id', \n",
    "                        right_on='author_id')\n",
    "    # Clean the titles and only keep rows that have a matching title in the HT\n",
    "    merged['simple_title'] = simple_title(merged['title'].fillna(''))\n",
    "    merged = merged[merged.simple_title.isin(clean_ht_titles)]\n",
    "    merged = merged.rename(columns={'name': 'author', 'new_name': 'author_formatted'})\n",
    "\n",
    "    # Add to a list of dataframes, which will be concatenated together at the end.\n",
    "    merged[cols_to_keep].to_csv(outf, mode='a' if i > 0 else 'w', compression='gzip')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "F22HcffPq2HC",
    "outputId": "0445146c-4ee7-4874-c8ff-f142a99a6170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (4,6,7,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112795"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the book title overlap\n",
    "clean_gr_titles = set(np.concatenate([df.simple_title.unique() for df in pd.read_csv(outf, compression='gzip', chunksize=100000)]))\n",
    "cleaned_title_overlap = clean_gr_titles.intersection(clean_ht_titles)\n",
    "len(cleaned_title_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "J3FSNIu0mA46",
    "outputId": "ae7cd33a-4683-4880-b7e0-cb8d1b157f58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (4,6,7,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "272407"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbooks = sum([df.shape[0] for df in pd.read_csv(outf, compression='gzip', chunksize=100000)])\n",
    "nbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_Z8ASevqqbY4",
    "outputId": "09090749-7a76-441f-efec-e709b9618e8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5028\n",
      "1 5477\n",
      "2 10739\n",
      "3 9488\n",
      "4 9082\n",
      "5 10084\n",
      "6 6286\n",
      "7 5274\n",
      "8 5658\n",
      "9 9218\n",
      "10 7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 8259\n",
      "12 6186\n",
      "13 8883\n",
      "14 6838\n",
      "15 7957\n",
      "16 3782\n",
      "17 3312\n",
      "18 5181\n",
      "19 2497\n",
      "20 1577\n",
      "21 5426\n",
      "22 7763\n",
      "23 8030\n",
      "24 5954\n",
      "25 5683\n",
      "26 11217\n",
      "27 18635\n",
      "28 12001\n",
      "29 5801\n",
      "30 7462\n",
      "31 4105\n",
      "32 4840\n",
      "33 4221\n",
      "34 5886\n",
      "35 17028\n",
      "36 2714\n",
      "37 6321\n",
      "38 10043\n",
      "39 3588\n",
      "40 7810\n",
      "41 4427\n",
      "42 6287\n",
      "43 5786\n",
      "44 5308\n",
      "45 2798\n",
      "46 2299\n",
      "47 6398\n",
      "48 7777\n",
      "49 5424\n",
      "50 3074\n",
      "51 1439\n",
      "52 2616\n",
      "53 4327\n",
      "54 4912\n",
      "55 7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 7141\n",
      "57 3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 4154\n",
      "59 5965\n",
      "60 5026\n",
      "61 4287\n",
      "62 3204\n",
      "63 3339\n",
      "64 2615\n",
      "65 9543\n",
      "66 5223\n",
      "67 3666\n",
      "68 3415\n"
     ]
    }
   ],
   "source": [
    "htreader = pd.read_csv('hathifiles.tsv.gz', sep='\\t', compression='gzip',\n",
    "                     chunksize=250000, names=headings,\n",
    "                     usecols=['htid', 'author', 'title', 'rights_date_used',\n",
    "                               'description'])\n",
    "outht = os.path.join(GRANT_FOLDER, 'ht_overlap.csv.gz')\n",
    "for i, htmeta in enumerate(htreader):\n",
    "    htmeta['clean_author'] = htmeta.author.fillna('').apply(clean_author)\n",
    "    htmeta = htmeta[htmeta.clean_author.isin(overlap)]\n",
    "    htmeta['simple_title'] = simple_title(htmeta.title.fillna(''))\n",
    "    htmeta = htmeta[htmeta.simple_title.isin(cleaned_title_overlap)]\n",
    "    print(i, htmeta.shape[0])\n",
    "    htmeta.to_csv(outht, mode='a' if i > 0 else 'w', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEHjb048uIYb"
   },
   "source": [
    "At this point, we have the Goodreads book data with titles and authors that may be in HT, and vice-versa. That was easier computation.\n",
    "\n",
    "Now that we have smaller datasets, we can align where the title+author are identical *together*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ceGGQXywrsfL",
    "outputId": "6237191e-5467-4f5f-fb02-65de6d7eb782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425198"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([df.shape[0] for df in pd.read_csv(outht, compression='gzip', chunksize=100000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n-zq-AmtYBM"
   },
   "outputs": [],
   "source": [
    "htdf = pd.read_csv(outht, compression='gzip')\n",
    "gtdf = pd.read_csv(outf, compression='gzip')\n",
    "\n",
    "# Combine author + title into a single string, for simplicity\n",
    "gtdf['code'] = gtdf['author_formatted'] + '__' + gtdf['simple_title']\n",
    "htdf['code'] = htdf['clean_author'] + '__' + htdf['simple_title']\n",
    "\n",
    "# Dictionaries are fast for lookups\n",
    "book_id_code_ref = gtdf.set_index('book_id').code.to_dict()\n",
    "htid_code_ref = htdf.set_index('htid').code.to_dict()\n",
    "\n",
    "# Do an inner join, to only keep where the codes overlap\n",
    "gtdf = gtdf.merge(htdf['code'].drop_duplicates(),\n",
    "                  how='inner', on='code')\n",
    "\n",
    "htdf = htdf.merge(gtdf['code'].drop_duplicates(),\n",
    "                  how='inner', on='code')\n",
    "\n",
    "# Overwrite earlier files\n",
    "gtdf.to_csv(outf, mode='w', compression='gzip')\n",
    "htdf.to_csv(outht, mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2lIMHrpADOQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse string of similar works into an actual list, and filter\n",
    "# to remove book_ids that are no longer in the dataset\n",
    "gtdf.book_id = gtdf.book_id.astype(int)\n",
    "unique_bookids = set(gtdf.book_id)\n",
    "def parse_list(x):\n",
    "    if x == \"[]\":\n",
    "        return []\n",
    "    else:\n",
    "        l = x[1:-1].split(', ')\n",
    "        return [int(y[1:-1]) for y in l]\n",
    "\n",
    "def filter_bookids(x):\n",
    "    m = set(x).intersection(unique_bookids)\n",
    "    return list(m)\n",
    "\n",
    "gtdf.similar_books = gtdf.similar_books.apply(parse_list).apply(filter_bookids)\n",
    "# Drop any rows without recommendations\n",
    "gtdf = gtdf[gtdf['similar_books'].apply(lambda x:len(x)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbFJ3peU8k17"
   },
   "outputs": [],
   "source": [
    "# Combine lists of similar_books, parse to use a+t codes. Also combine book_ids\n",
    "def concat(x):\n",
    "    sim_books = [id for l1 in x.similar_books.tolist() for id in l1]\n",
    "    sim_books = list(set(sim_books))\n",
    "    sim_book_codes = [book_id_code_ref[str(x)] for x in sim_books]\n",
    "    return pd.Series({'similar_books':sim_book_codes, 'book_ids': x.book_id.tolist()})\n",
    "\n",
    "by_code = gtdf.reset_index().groupby(['code'])[['book_id', 'similar_books']].apply(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "colab_type": "code",
    "id": "KguZZGgdN0aB",
    "outputId": "906e28a4-f54c-4138-975c-254aa8a7694c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar_books</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>htid</th>\n",
       "      <th>similar_htids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Williams, Charles__theplaceofthelion</th>\n",
       "      <td>[Carpenter, Humphrey__theinklings]</td>\n",
       "      <td>[143226, 1732118]</td>\n",
       "      <td>[mdp.39015066681407, uc1.$b391206, mdp.3901500...</td>\n",
       "      <td>[mdp.39015002385162, mdp.39015002385006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walton, Anthony__mississippi</th>\n",
       "      <td>[Barfield, Owen__historyinenglishwords, Lerer,...</td>\n",
       "      <td>[178784]</td>\n",
       "      <td>[mdp.39015037261107, uva.x002712367]</td>\n",
       "      <td>[mdp.39076006122779, inu.30000037449935, mdp.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tindall, Blair__mozartinthejungle</th>\n",
       "      <td>[Piston, Walter__harmony]</td>\n",
       "      <td>[34684275, 24998, 24752265, 19793474, 1473248]</td>\n",
       "      <td>[mdp.39015061459205]</td>\n",
       "      <td>[uc1.b4325083]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mayer, Mercer__justgrandpaandme</th>\n",
       "      <td>[Piven, Hanoch__mydogisassmellyasdirtysocks, D...</td>\n",
       "      <td>[633709]</td>\n",
       "      <td>[pst.000032698923]</td>\n",
       "      <td>[pst.000033005577, pst.000061597174]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Efremov, Ivan Antonovich__andromedaaspaceagetale</th>\n",
       "      <td>[Tolstoy, Aleksey Nikolayevich__aelita, Harris...</td>\n",
       "      <td>[26823107]</td>\n",
       "      <td>[mdp.39015046367887, mdp.39015038020460, uc1.$...</td>\n",
       "      <td>[pst.000000948081, uc1.b3462022, uiug.30112093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiell, Charlotte__1000chairs</th>\n",
       "      <td>[Macaulay, David__buildingbig]</td>\n",
       "      <td>[1083029]</td>\n",
       "      <td>[mdp.39015056309894]</td>\n",
       "      <td>[mdp.39015049724571]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hyde, Catherine Ryan__electricgod</th>\n",
       "      <td>[Howatch, Susan__thewonderworker, Tiffany, Car...</td>\n",
       "      <td>[217450, 16124215]</td>\n",
       "      <td>[mdp.49015003325322]</td>\n",
       "      <td>[uva.x006112947, inu.30000103141119, mdp.39015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proust, Marcel__thecompleteshortstoriesofmarcelprou</th>\n",
       "      <td>[Beckett, Samuel__proust, Deleuze, Gilles__pro...</td>\n",
       "      <td>[1770405, 28394]</td>\n",
       "      <td>[mdp.39015050755985, mdp.39015070703452]</td>\n",
       "      <td>[mdp.39015005323111, mdp.39015008690995, mdp.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Duras, Marguerite__lapluiedété</th>\n",
       "      <td>[Cendrars, Blaise__gold, D'Aguiar, Fred__feedi...</td>\n",
       "      <td>[1147388]</td>\n",
       "      <td>[mdp.39015017018352]</td>\n",
       "      <td>[mdp.39015008513734, uc1.b3757121, mdp.3901506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Newman, John Henry__apologiaprovitasua</th>\n",
       "      <td>[Belloc, Hilaire__thegreatheresies, Adam, Karl...</td>\n",
       "      <td>[43958, 1432861, 982471, 20613395, 18187394, 3...</td>\n",
       "      <td>[uc2.ark:/13960/t39z92w3d, mdp.39015008498209,...</td>\n",
       "      <td>[pst.000023794283, uc1.$b51909, uva.x000531104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        similar_books  ...                                      similar_htids\n",
       "code                                                                                                   ...                                                   \n",
       "Williams, Charles__theplaceofthelion                               [Carpenter, Humphrey__theinklings]  ...           [mdp.39015002385162, mdp.39015002385006]\n",
       "Walton, Anthony__mississippi                        [Barfield, Owen__historyinenglishwords, Lerer,...  ...  [mdp.39076006122779, inu.30000037449935, mdp.3...\n",
       "Tindall, Blair__mozartinthejungle                                           [Piston, Walter__harmony]  ...                                     [uc1.b4325083]\n",
       "Mayer, Mercer__justgrandpaandme                     [Piven, Hanoch__mydogisassmellyasdirtysocks, D...  ...               [pst.000033005577, pst.000061597174]\n",
       "Efremov, Ivan Antonovich__andromedaaspaceagetale    [Tolstoy, Aleksey Nikolayevich__aelita, Harris...  ...  [pst.000000948081, uc1.b3462022, uiug.30112093...\n",
       "Fiell, Charlotte__1000chairs                                           [Macaulay, David__buildingbig]  ...                               [mdp.39015049724571]\n",
       "Hyde, Catherine Ryan__electricgod                   [Howatch, Susan__thewonderworker, Tiffany, Car...  ...  [uva.x006112947, inu.30000103141119, mdp.39015...\n",
       "Proust, Marcel__thecompleteshortstoriesofmarcel...  [Beckett, Samuel__proust, Deleuze, Gilles__pro...  ...  [mdp.39015005323111, mdp.39015008690995, mdp.3...\n",
       "Duras, Marguerite__lapluiedété                      [Cendrars, Blaise__gold, D'Aguiar, Fred__feedi...  ...  [mdp.39015008513734, uc1.b3757121, mdp.3901506...\n",
       "Newman, John Henry__apologiaprovitasua              [Belloc, Hilaire__thegreatheresies, Adam, Karl...  ...  [pst.000023794283, uc1.$b51909, uva.x000531104...\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htid_by_code = htdf.groupby('code')['htid'].apply(lambda x: x.tolist())\n",
    "by_code = by_code.merge(htid_by_code, left_index=True, right_index=True)\n",
    "by_code['similar_htids'] = by_code.similar_books.apply(lambda x: list(set([l for z in [htid_by_code[y] for y in x] for l in z])))\n",
    "by_code.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgJ50Yl7UZZ0"
   },
   "source": [
    "# Save Data\n",
    "\n",
    "This data fits better as JSON, because of the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOLaFaWmUheo"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "simspath = os.path.join(GRANT_FOLDER, 'good_reads_sims.json.gz')\n",
    "with gzip.GzipFile(simspath, 'w') as fout:\n",
    "    fout.write(by_code.reset_index().to_json(orient='records', lines=True).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ay4SrRm9z2Cg"
   },
   "source": [
    "Potentially also useful - the data exploded into all left/right permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "id": "0mPlEswNztts",
    "outputId": "2ced2306-091d-4283-8dfa-7d1593a5a116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('pairwise_gr_stats.json.gz', mode='w') as f:\n",
    "    f.write('left\\tright\\n'.encode('utf-8'))    \n",
    "    for i, (ind, row) in enumerate(by_code.iterrows()):\n",
    "        try:\n",
    "            pairs = [(htid, htid2) for htid in row['htid'] for htid2 in row['similar_htids']]\n",
    "            for pair in pairs:\n",
    "                out = json.dumps(dict(left=pair[0], right=pair[1], relationship='GRSIM')) +'\\n'\n",
    "                f.write(out.encode('utf-8'))\n",
    "        except:\n",
    "            print('Error with ', pair)\n",
    "        if i % 1000 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulJ4L9xEKHN_"
   },
   "source": [
    "If trimming the set downstream, sample then sort by left, rather than leaving unordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSXZO21Jd1E4"
   },
   "source": [
    "#### Workspace\n",
    "\n",
    "Reloading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdMFqdI1dzq7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "simspath = os.path.join(GRANT_FOLDER, 'good_reads_sims.json.gz')\n",
    "df = pd.read_json(simspath, compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "PmdS1ixteX7H",
    "outputId": "b007909e-eb25-4f75-a57f-5ac6db043ede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91655"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the HTIDs that needs to be crunched for training\n",
    "unique_htids = set([htid for htids in df.htid for htid in htids] + [htid for htids in df.similar_htids for htid in htids])\n",
    "len(unique_htids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94iu1QatfNag"
   },
   "outputs": [],
   "source": [
    "pd.Series(list(unique_htids)).sample(frac=1).to_csv(os.path.join(GRANT_FOLDER, 'goodreads_htids.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "GoodReadsBookAlignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
